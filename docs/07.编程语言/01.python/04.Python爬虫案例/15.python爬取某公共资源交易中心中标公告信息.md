年终为了统计今年公司的中标情况，同时对比同行竞争对手的中标数量和金额。以下以地方公共资源交易网为例。

**思路如下：** 

使用python，通过requests读取网页，首先读取中标公告列表，通过更换链接的pageID即可循环翻页，最终把70页内的公告列表爬出来。

再使用pyquery解析每一页的HTML，获取每个公告的标题、时间以及最重要的公告正文链接，链接格式是固定的，变的只是其中的ID，通过更换这个ID即可循环读取每个公告链接的正文文本。

逐个公告正文读取下来后，使用正则表达式RE根据关键词，获取关键信息，如供应商名称、中标金额等。因为正文的不规范性，这里需要多次调试，排除干扰字符，多个条件循环判断等。

最后把获取到的信息通过pandas合并列表成dataframe，并写入到csv。

**代码如下：** 

```py
import requests
from pyquery import PyQuery as pq
import pandas as pd
import re

# 创建一个空的数据矩阵
data=pd.DataFrame()

# 创建多个空列表
date_list=[]
name_list=[]
ID_list=[]
supplier_list=[]
money_list=[]

# 定义头部和代理
headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'}
proxies = {
  "http": "http://xxx.xxx.xxx:xx",
  "https": "http://xxx.xxx.xxx:xx",
}

# 初始化session
s = requests.session()  

# 处理文字页面子程序，后面循环调用
def get_page(articalID):
        # 给URL放置传进来的ID参数
	url_page='https://ggzyjy.zs.gov.cn/Application/NewPage/ggnr.jsp?articalID='+articalID+'&nodeID=55'
	html_page=s.get(url=url_page,headers=headers,proxies=proxies).text  #先获取html文本，记得加上代理（如有）

	html_page=html_page.replace('html xmlns=','') 	# 由于页面有多个HTML，影响解析，因此直接删除掉
	html_page=html_page.replace('html','') 

	doc=pq(html_page,parser='html') 	# 再用pyquery解析html文本

	doc_span=doc('.articalDiv p').text()  #按class和标签索引到元素
	doc_span = "".join(doc_span.split())  # 去除latin类型的空格

	doc_span=doc_span.replace('（元）','') 	# 替换掉一些影响匹配的字符
	doc_span=doc_span.replace('：','') 

	# 下面开始爬公司名称

	#  [\u3002\uff1b\uff0c\uff1a\u201c\u201d\uff08\uff09\u3001\uff1f\u300a\u300b] 
	#  上面代表中文符号。 ； ， ： “ ”（ ） 、 ？ 《 》 这些标点符号。
	#  [\u4e00-\u9fa5] 代表中文文字
	#  (.*?)表示提取中间所有的内容

	try:
		supplier = re.findall(r'供应商名称(.*?)[\u3002\uff1b\uff0c\uff1a\u201c\u201d\uff08\uff09\u3001\uff1f\u300a\u300b]',doc_span)
		# print('1、',supplier)
		if supplier == []:	# 如果为空
			supplier=''
		else:	# 不为空
			supplier1 = re.findall(r'(.*?)供应商地址' , supplier[0])
			# print('2、',supplier1)
			if supplier1 == []:	# 如果后缀1是空
				supplier2 = re.findall(r'(.*?)法人代表' , supplier[0])
				# print('3、',supplier2)
				if supplier2 == []: # # 如果后缀2是空
					supplier3 = re.findall(r'(.*?)地址' , supplier[0])	
					# print('4、',supplier)
					if supplier3 == []: # # 如果后缀3是空
						supplier=supplier[0]
					else:
						supplier=supplier3[0]
				else:
					supplier=supplier2[0]
			else:
				supplier=supplier1[0]
		supplier_list.append(supplier)
	except:
		supplier_list.append('')

	# 下面开始爬中标金额
	try:
		money = re.findall(r'中标（成交）金额(.*?)[\u3002\uff1b\uff0c\uff1a\u201c\u201d\uff08\uff09\u3001\uff1f\u300a\u300b\u4e00-\u9fa5]',doc_span)
		# print('1、',money)
		if money == []:	# 如果为空
			money = re.findall(r'项目预算金额(.*?)[\u3002\uff1b\uff0c\uff1a\u201c\u201d\uff08\uff09\u3001\uff1f\u300a\u300b\u4e00-\u9fa5]' , doc_span)
			# print('2、',money)
			if money == []:	# 如果还是空
				money=''
			else:
				pass
		else:
			pass
		money=money[0].strip('：').strip(':').strip('￥')
		money_list.append(money)
	except:
		money_list.append('')

# 这里才是主程序，爬主页面获取articalID
for page_id in range(1,72):	# 从1开始到72页
	url='https://ggzyjy.zs.gov.cn/Application/NewPage/PageSubItem.jsp?page='+str(page_id)+'&node=55'
	html=s.get(url=url,headers=headers,proxies=proxies).text  #先获取html文本

	html=html.replace('html xmlns=','')  # 因为html有多个，影响解析，这里要全部去掉
	html=html.replace('html','') 

	doc=pq(html,parser='html') # 再用pyquery解析html文本

	dates=doc('.nav_list ul span')  # 通过属性和标签定位到要的信息
	names=doc('.nav_list ul a')

        # 由于pyquery定位元素后返回的是一个元组，因此需要循环读取出来装到列表中
	for date in dates:
		date_list.append(date.text)	# 日期
	for name in names:
		name_list.append(name.text)	# 链接

	
	for name in names.items():  # ID
		href=name.attr('href')
		try:
			articalID= re.findall(r'articalID=(.*)',href)[0]	# 通过链接获取ID
		except:
			articalID='206204'		# ID为空的时候随便找个ID替代，避免出错
		ID_list.append(articalID)
		get_page(articalID)		# 调用子程序，跑ID对应页面的内容

# 合成到pandas的DataFrame
data['date']=date_list
data['name']=name_list
data['ID']=ID_list
data['supplier']=supplier_list
data['money']=money_list

# 写入到csv，注意编码改为：utf-8
data.to_csv(r'output_file.csv',encoding="utf_8_sig")
``` 
 [https://zhuanlan.zhihu.com/p/328294139](https://zhuanlan.zhihu.com/p/328294139)
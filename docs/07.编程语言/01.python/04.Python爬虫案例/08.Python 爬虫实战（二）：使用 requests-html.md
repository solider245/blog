---
title: Python 爬虫实战（二）：使用 requests-html
date: 2020-10-13 04:52:39
permalink: /pages/95b22a/
categories:
  - python
  - Python爬虫案例
tags:
  - 
---
[Python 爬虫实战（一）：使用 requests 和 BeautifulSoup](http://wuxiaolong.me/2017/12/10/PythonCrawler1/)，我们使用了 requests 做网络请求，拿到网页数据再用 BeautifulSoup 解析，就在前不久，[requests](https://github.com/requests/requests) 作者 [kennethreitz](https://github.com/kennethreitz) 出了一个新库 [requests\-html](https://github.com/kennethreitz/requests-html)，Pythonic HTML Parsing for Humans™，它可以用于解析 HTML 文档的。requests\-html 是基于现有的框架 PyQuery、Requests、lxml 等库进行了二次封装，更加方便开发者调用。

# [](#安装 "安装")安装

Mac：

|

1

 |

pip3 install requests\-html

 |

Windows：

|

1

 |

pip install requests\-html

 |

# [](#实例 "实例")实例

![](http://wuxiaolong.me/images/PythonCrawler1.png)

代码撸多了，让我们看会妹纸，爬的网站我选的是 [http://www.win4000.com/zt/xinggan.html](http://www.win4000.com/zt/xinggan.html) ，打开网站，观察到这是个列表，图片是缩略图，要想保存图片到本地，当然需要高清大图，因此得进入列表详情，进一步解析，完整代码如下：

|

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

 |

from requests\_html import HTMLSession

import requests

import time

session = HTMLSession()

\# 解析图片列表

def get\_girl\_list():

    \# 返回一个 response 对象

    response = session.get('[http://www.win4000.com/zt/xinggan.html](http://www.win4000.com/zt/xinggan.html)')  \# 单位秒数

    content = response.html.find('div.Left\_bar', first=True)

    li\_list = content.find('li')

    for li in li\_list:

        url = li.find('a', first=True).attrs\['href'\]

        get\_girl\_detail(url)

\# 解析图片详细

def get\_girl\_detail(url):

    \# 返回一个 response 对象

    response = session.get(url)  \# 单位秒数

    content = response.html.find('div.scroll\-img\-cont', first=True)

    li\_list = content.find('li')

    for li in li\_list:

        img\_url = li.find('img', first=True).attrs\['data\-original'\]

        img\_url = img\_url\[0:img\_url.find('\_')\] + '.jpg'

        print(img\_url + '.jpg')

        save\_image(img\_url)

\# 保持大图

def save\_image(img\_url):

    img\_response = requests.get(img\_url)

    t = int(round(time.time() \* 1000))  \# 毫秒级时间戳

    f = open('/Users/wuxiaolong/Desktop/Girl/%d.jpg' % t, 'ab')  \# 存储图片，多媒体文件需要参数b（二进制文件）

    f.write(img\_response.content)  \# 多媒体存储content

    f.close()

if \_\_name\_\_ == '\_\_main\_\_':

    get\_girl\_list()

 |

代码就这么多，是不是感觉很简单啊。

**说明：**

1、requests\-html 与 BeautifulSoup 不同，可以直接通过标签来 find，一般如下：
标签
标签.someClass
标签#someID
标签\[target=\_blank\]
参数 first 是 True，表示只返回 Element 找到的第一个，更多使用：[http://html.python\-requests.org/](http://html.python-requests.org/) ；

2、这里保存本地路径 `/Users/wuxiaolong/Desktop/Girl/`我写死了，需要读者改成自己的，如果直接是文件名，保存路径将是项目目录下。

# [](#遗留问题 "遗留问题")遗留问题

示例所爬网站是分页的，没有做，可以定时循环来爬妹纸哦，有兴趣的读者自己玩下。

# [](#参考 "参考")参考

[requests\-html](https://github.com/kennethreitz/requests-html)

[今天用了一下Requests\-HTML库（Python爬虫）](http://blog.csdn.net/qq_31845675/article/details/79501868)

# 联系作者

我的微信公众号：吴小龙同学，欢迎关注交流，公号回复关键字「1024」有惊喜哦。
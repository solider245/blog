---
title: 爬虫：requests 爬取时对已经抓取的 url 以及 post data 进行 md5 处理，并保存到 csv 以用于去重过滤
date: 2020-10-13 04:52:39
permalink: /pages/195a3b/
categories:
  - python
  - Python爬虫案例
tags:
  - 
---
# 1\. 需求：

*   在 requests 爬取网页的过程中，需要将已经爬过的 url（以及 post data）保存起来，用于爬虫重启等场景的过滤去重；
*   在向目标 url 发送访问请求之前，先查看 url 是否已经被抓取过，吐过已经被抓取过则不再抓取；

# 2\. 实现：

```python
import csv
import hashlib
import os
import requests

class DupfilteTool(object):

    def __init__(self):
        self.path = 'success_download_url_md5.csv'
        if not os.path.exists(self.path):
            # os.mknod(self.path)
            open(self.path, 'a')

    def save_success_url_md5(self, url, data=None):
        '''
        保存已经成功抓取的网页 url（包括 data）的 md5 值
        :params url: 要保存的 url 地址
        :params data: 当访问请求类型为 post 时，url 是不变的，变化的是需要提交的 data
        :return:
        '''
        if data:
            url += str(data)
        md5_result = hashlib.md5(url.encode('utf-8')).hexdigest()
        path = 'success_download_url_md5.csv'
        with open(path, 'a', newline='') as file:
            csv_file = csv.writer(file)
            csv_file.writerow([md5_result])
            if data:
                print('success download url {} with data {}'.format(url, data))
            else:
                print('success download url {}'.format(url))

    def read_downloaded_url_md5_list(self):
        '''
        读取所有已经成功抓取的 url（包括 data）的 MD5 值
        :return:
        '''
        f = open(self.path, 'r')
        csv_reader = csv.reader(
            _.replace('\x00', '') for _ in f)  # _.replace('\x00', '') for _ in f 用于解决 xls 文件保存为 csv 是出现阅读的异常报错问题
        url_md5_list = [x[0] for x in list(csv_reader)]
        return url_md5_list

    def is_url_downloaded(self, url, data=None):
        '''
        判断当前 url（包括 data）是否已经被下载
        :params url: url 地址
        :params data: 当访问请求类型为 post 时，url 是不变的，变化的是需要提交的 data
        :return:
        '''
        if data:
            url += str(data)
        url_md5 = hashlib.md5(url.encode('utf-8')).hexdigest()
        if url_md5 in self.read_downloaded_url_md5_list():
            if data:
                print('already downloaded url : {} with data : {}'.format(url, data))
            else:
                print('already downloaded url {}'.format(url))
            return True
        return False

#-----------------------------------------------------------------
# 发送 get 请求用法
#-----------------------------------------------------------------
url_list = [
    'https://www.xxx-1.com'
    'https://www.xxx-2.com'
    'https://www.xxx-3.com'
    'https://www.xxx-4.com'
]

for url in url_list:
    dupefilte_tool = DupfilteTool()

    # 判断 url 是否已经爬取过，如果已经爬取过就不再爬取
    if dupefilte_tool.is_url_downloaded(url):
        continue
    else:
        response = requests.get(url=url)
        dupefilte_tool.save_success_url_md5(url)

#-----------------------------------------------------------------
# 发送 post 请求用法
#-----------------------------------------------------------------
url = 'http://www.xxx.com'
post_data = {
    'key_1': 'value_1',
    'key_2': 'value_2'
}
dupefilte_tool = DupfilteTool()
if not dupefilte_tool.is_url_downloaded(url, post_data):
    response = requests.get(url=url, data=post_data)
    dupefilte_tool.save_success_url_md5(url, post_data)
else:
    pass
```
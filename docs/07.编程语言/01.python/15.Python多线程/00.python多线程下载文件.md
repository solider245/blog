## 序言
kaggle 前一段时间公布了imaterialist\-product的竞赛，闲着无事就下载了一下数据集看一下，他的数据集是给出了每个图片的类别，id与url链接地址，下载需要从json格式文件中读取这些需要的信息，然后编写程序进行下载

## 主要代码
首先就直接读出url地址，利用python的urllib包，单线程进行下载，速度特别慢，然后就思考采用多线程下载提高速度

单线程读取json文件并下载

```python
#! /usr/bin/env python
# -*- coding: utf-8 -*-

import os
import  json as js
import time
import urllib.request
import socket
import urllib2
#import request

socket.setdefaulttimeout(30)  # timout=30s   时间长不反应就进行下一个文件的下载

json_path="json/train.json"
image_save="imertialist_image/train"
if not os.path.exists(image_save):
		os.makedirs(image_save)
for img_cla in range(2019):        ####数据共分为2019类
	if not os.path.exists(os.path.join(image_save,str(img_cla))):
		os.makedirs(os.path.join(image_save,str(img_cla)))

f=open(json_path)   #读取json文件
setting=js.load(f)

images=setting["images"]
for  img in images:
	img_url=img["url"]
	img_id=img["id"]
	img_class=str(img["class"])
#	if not os.path.exists(os.path.join(image_save,img_class)):
#		os.makedirs(os.path.join(image_save,img_class))
	if img["class"]>36:
		print(img_url)
		try:
			urllib.request.urlretrieve(img_url, os.path.join(os.path.join(image_save,img_class),img_id))
		except urllib.error.HTTPError,e:
    		continue
		except urllib2.error.URLError,e:
    		continue
		except socket.timeout:
			continue

		print(img_class)

#	break

f.close()
print("end")
```

下载速度特别慢，考虑采用python多线程下载，速度特别快

```py
# -*- coding: utf-8 -*-
import os
from contextlib import closing
import threading
import requests
import json as js

headers = {
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'
}

#url 文件夹
json_path="json/train.json"

#输出文件夹
out_dir = "train"
#线程数
thread_num = 400
#http请求超时设置
timeout = 30

if not os.path.exists(out_dir):
    os.makedirs(out_dir)
for img_cla in range(2019):  #一共2019类，保存在不同文件夹中
    if not os.path.exists(os.path.join(out_dir,str(img_cla))):
        os.makedirs(os.path.join(out_dir,str(img_cla)))

def download(img_url, img_name, img_class):
    if os.path.isfile(os.path.join(os.path.join(out_dir, str(img_class)), img_name)):
        return    ####如果之前下载过这个文件，就跳过
    with closing(requests.get(img_url, stream=True, headers=headers, timeout=timeout)) as r:
        rc = r.status_code
        if 299 < rc or rc < 200:
            print ('returnCode%s\t%s' % (rc, img_url))
            return
        content_length = int(r.headers.get('content-length', '0'))
        if content_length == 0:
            print ('size0\t%s' % img_url)
            return
        try:
            with open(os.path.join(os.path.join(out_dir, str(img_class)), img_name), 'wb') as f:
                for data in r.iter_content(1024):
                    f.write(data)
        except:
            print('savefail\t%s' % img_url)

def get_img_url_generate():
    imgs=[]
    with open(json_path,'r') as f:
        setting=js.load(f)
        images=setting["images"]
        for img in images:
            imgs=[]
            img_url=img['url']
            img_id=img['id']
            img_class=img['class']
            imgs.append(img_url)
            imgs.append(img_id)
            imgs.append(img_class)
            try:
                if img_url:
                    yield imgs
            except:
                break

lock = threading.Lock()
def loop(imgs):
    print ('thread %s is running...' % threading.current_thread().name)

    while True:
        try:
            with lock:
                img_url,img_id,img_class = next(imgs)
                print(img_class)
        except StopIteration:
            break
        try:

            download(img_url, img_id, img_class)
        except:
            print ('exceptfail\t%s' % img_url)
    print ('thread %s is end...' % threading.current_thread().name)

imgs = get_img_url_generate()

for i in range(0, thread_num):
    t = threading.Thread(target=loop, name='LoopThread %s' % i, args=(imgs,))
    t.start()

```

## 其他案例
### 下载百度首页的图片
#### 思路

1.  先从http head中获取文件的大小
2.  将大小分隔成若干份(一个线程下载一份)
3.  通过seek将下载的块的内容写到文件的对应的位置，对每一个线程下载的数据块进行拼接

#### 代码(下载百度首页的图片为例)

```ruby
import requests
import threading

class downloader:
    def __init__(self):
        self.url = "https://ss0.bdstatic.com/5aV1bjqh_Q23odCf/static/superman/img/logo/bd_logo1_31bdc765.png"
        self.num = 8
        self.name = "baidu.png"
        r = requests.head(self.url)
        # 获取文件大小
        self.total = int(r.headers['Content-Length'])
        print self.total

    # 获取每个线程下载的区间
    def get_range(self):
        ranges = []
        offset = int(self.total/self.num)
        for i in range(self.num):
            if i == self.num-1:
                ranges.append((i*offset,''))
            else:
                ranges.append((i*offset,(i+1)*offset))
        return ranges  # [(0,100),(100,200),(200,"")]

    # 通过传入开始和结束位置来下载文件
    def download(self,start,end):
        headers = {'Range':'Bytes=%s-%s'%(start,end),'Accept-Encoding':'*'}
        res = requests.get(self.url,headers=headers)
        print "%s-%s download success"%(start,end)
        # 将文件指针移动到传入区间开始的位置
        self.fd.seek(start)
        self.fd.write(res.content)

    def run(self):
        self.fd = open(self.name,"wb")

        thread_list = []
        n = 0

        for ran in self.get_range():
            # 获取每个线程下载的数据块
            start,end = ran
            n += 1
            thread = threading.Thread(target=self.download,args=(start,end))
            thread.start()
            thread_list.append(thread)

        for i in thread_list:
            # 设置等待，避免上一个数据块还没写入，下一数据块对文件seek，会报错
            i.join()

        self.fd.close()

if __name__ == "__main__":
    downloader().run()
```
### Python多线程下载文件和注释以及带有进度条显示的单线程下载,注意事项

发表时间：2020\-05\-20

批量爬虫下载时,单线程下载文件有时慢有时快有那以稳定,有点浪费了我200M的带宽嘿嘿。写一个简单的多线程分块下载文件工具,从网上找了几个代码,试了一下发现有些奇怪的问题，刚开始不知道线程锁,线程安全,颇为苦恼的查了一些资料,最终调试成功，工具还是有一些不完美的，比如没有用Sterm=True模式写硬盘,而是从内存中写入文件,下载文件超过内存的大文件时,内存较小的有可能会报错。不过稍改一下也就可以了。代码备注比较完整，可以直接使用。给自已记录一下，也留给需要的童鞋们。

需要注意的就是线程锁的事情,局部变量是线程安全的。网上有的代码用的是类全局变量打开文件但在多线程中并未加锁，会导致文件有一定几率出现大小和源文件不同，即使文件大小相同,MD5值也不同,中间有一段是坏的，在图片和音频中可能只是其中一段损坏,exe,rar之类的就直接打不开了。 附上单线程下载(带进度条显示)和多线程代码的时间比较，实测多线程能有效提升下载效率，效果还是比较不错的。

```py
# -!- coding: utf-8 -!-

import os, requests, time, threading
from queue import Queue
# import logging
from tqdm import tqdm

# lock = threading.Lock()           # 改成局部变量写文件,就无需线程锁
# ===================================================================================================================

# 单线程下载文件,download_from_url(url,"./aa.mp3")
def single_thread_download(url, dst):
        """
        @param: url to download file
        @param: dst place to put the file
        """
        #file_size = int(urlopen(url).info().get('Content-Length', -1))
        file_size = int(requests.head(url).headers['Content-Length'])
        if os.path.exists(dst):
            first_byte = os.path.getsize(dst)
        else:
            first_byte = 0
        if first_byte >= file_size:
            return file_size
        header = {"Range": "bytes=%s-%s" % (first_byte, file_size)}
        pbar = tqdm(
            total=file_size, initial=first_byte,
            unit='B', unit_scale=True, desc=url.split('/')[-1])
        req = requests.get(url, headers=header, stream=True)
        with(open(dst, 'ab')) as f:
            for chunk in req.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    pbar.update(1024)
        pbar.close()
        return file_size

class ManyThreadDownload:
    def __init__(self, num=10):
        self.num = num              # 线程数,默认10
        self.url = ''               # url
        self.name = ''              # 目标地址
        self.total = 0              # 文件大小

    # 获取每个线程下载的区间
    def get_range(self):
        ranges = []
        offset = int(self.total/self.num)
        for i in range(self.num):
            if i == self.num-1:
                ranges.append((i*offset, ''))
            else:
                ranges.append(((i * offset), (i + 1) * offset - 1))
        return ranges               # [(0,99),(100,199),(200,"")]

    # 通过传入开始和结束位置来下载文件
    def download(self, ts_queue):
        while not ts_queue.empty():
            start_, end_ = ts_queue.get()
            headers = {
                'Range': 'Bytes=%s-%s' % (start_, end_),
                'Accept-Encoding': '*'
                }
            flag = False
            while not flag:
                try:
                    # 设置重连次数
                    requests.adapters.DEFAULT_RETRIES = 10
                    # s = requests.session()            # 每次都会发起一次TCP握手,性能降低，还可能因发起多个连接而被拒绝
                    # # 设置连接活跃状态为False
                    # s.keep_alive = False
                    # 默认stream=false,立即下载放到内存,文件过大会内存不足,大文件时用True需改一下码子
                    res = requests.get(self.url, headers=headers)
                    res.close()                         # 关闭请求  释放内存
                except Exception as e:
                    print((start_, end_, "出错了,连接重试:%s", e, ))
                    time.sleep(1)
                    continue
                flag = True

            print("\r", ("%s-%s download success" % (start_, end_)), end="", flush=True)
            # with lock:
            with open(self.name, "rb+") as fd:
                fd.seek(start_)
                fd.write(res.content)
            # self.fd.seek(start_)                                        # 指定写文件的位置,下载的内容放到正确的位置处
            # self.fd.write(res.content)                                  # 将下载文件保存到 fd所打开的文件里

    def run(self, url, name):
        self.url = url
        self.name = name
        self.total = int(requests.head(url).headers['Content-Length'])
        # file_size = int(urlopen(self.url).info().get('Content-Length', -1))
        file_size = self.total
        if os.path.exists(name):
            first_byte = os.path.getsize(name)
        else:
            first_byte = 0
        if first_byte >= file_size:
            return file_size

        self.fd = open(name, "wb")                   # 续传时直接rb+ 文件不存在时会报错,先wb再rb+
        self.fd.truncate(self.total)                 # 建一个和下载文件一样大的文件,不是必须的,stream=True时会用到
        self.fd.close()
        # self.fd = open(self.name, "rb+")           # 续传时ab方式打开时会强制指针指向文件末尾,seek并不管用,应用rb+模式
        thread_list = []
        ts_queue = Queue()                           # 用队列的线程安全特性，以列表的形式把开始和结束加到队列
        for ran in self.get_range():
            start_, end_ = ran
            ts_queue.put((start_, end_))

        for i in range(self.num):
            t = threading.Thread(target=self.download, name='th-' + str(i), kwargs={'ts_queue': ts_queue})
            t.setDaemon(True)
            thread_list.append(t)
        for t in thread_list:
            t.start()
        for t in thread_list:
            t.join()                                # 设置等待，全部线程完事后再继续

        self.fd.close()

if __name__ == '__main__':

    start = time.perf_counter()
    single_thread_download('http://wechatapppro-1252524126.file.myqcloud.com/appsVcR0oga2638/audio/ka239k150wxmes16vkc.mp3', './1.mp3')
    end = time.perf_counter()
    print('[Message] Running time: %s Seconds' % (end - start))

    many_thread_download = ManyThreadDownload()
    start = time.perf_counter()
    many_thread_download.run('http://wechatapppro-1252524126.file.myqcloud.com/appsVcR0oga2638/audio/ka239k150wxmes16vkc.mp3', './2.mp3')
    end = time.perf_counter()
    print('\n')
    print('[Message] Running time: %s Seconds' % (end - start))

```
### 下载文件
```py
#!/root/.pyenv/shims/python
# -*- coding: UTF-8 -*-
import sys
import requests
import threading
import datetime
#传入的命令行参数，要下载文件的url
url = sys.argv[1]
def Handler(start, end, url, filename):
    headers = {'Range': 'bytes=%d-%d' % (start, end)}
    r = requests.get(url, headers=headers, stream=True,timeout=5)
    # 写入文件对应位置
    with open(filename, "r+b") as fp:
        fp.seek(start)
        var = fp.tell()
        fp.write(r.content)
def download_file(url, num_thread = 10):
    r = requests.head(url)
    try:
        file_name = url.split('/')[-1]
        file_size = int(r.headers['content-length'])   # Content-Length获得文件主体的大小，当http服务器使用Connection:keep-alive时，不支持Content-Length
    except:
        print("检查URL，或不支持对线程下载")
        return
    #  创建一个和要下载文件一样大小的文件
    fp = open(file_name, "wb")
    fp.truncate(file_size)
    fp.close()
    # 启动多线程写文件
    part = file_size // num_thread  
    # 如果不能整除，最后一块应该多几个字节
    for i in range(num_thread):
        start = part * i
        if i == num_thread - 1:   # 最后一块
            end = file_size
        else:
            end = start + part
        t = threading.Thread(target=Handler, kwargs={'start': start, 'end': end, 'url': url, 'filename': file_name})
        t.setDaemon(True)
        t.start()
    # 等待所有线程下载完成
    main_thread = threading.current_thread()
    for t in threading.enumerate():
        if t is main_thread:
            continue
        t.join()
    print('%s 下载完成' % file_name)
if __name__ == '__main__':
    start = datetime.datetime.now().replace(microsecond=0)  
    download_file(url)
    end = datetime.datetime.now().replace(microsecond=0)
    print("用时: ", end='')
    print(end-start)
```

## 参考文章
[第八篇 python多线程下载文件 - 知乎](https://zhuanlan.zhihu.com/p/63982089)
[python多线程下载文件 - 简书](https://www.jianshu.com/p/5c71ad87a52c)
[Python多线程下载文件和注释以及带有进度条显示的单线程下载,注意事项](https://www.pythonf.cn/read/112198)